{"cells":[{"cell_type":"markdown","metadata":{"id":"Tt5FpWrgNyJF"},"source":["## Setup the StreetFighter environment"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:36:34.204629Z","iopub.status.busy":"2023-01-01T06:36:34.203951Z","iopub.status.idle":"2023-01-01T06:39:28.050471Z","shell.execute_reply":"2023-01-01T06:39:28.048093Z","shell.execute_reply.started":"2023-01-01T06:36:34.204487Z"},"id":"0DD3SS8qNyJH","outputId":"275802c8-aee0-4e4b-a8eb-a4557fe0721f","trusted":true},"outputs":[],"source":["%pip install --upgrade pip\n","%pip install gym==0.21.0\n","%pip install gym-retro\n","%pip install retrowrapper\n","%pip install opencv-python\n","%pip install matplotlib\n","%pip install torch \n","%pip install stable-baselines3[extra]\n","%pip install stable-baselines3\n","%pip install optuna\n","%pip install tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:28.056623Z","iopub.status.busy":"2023-01-01T06:39:28.055127Z","iopub.status.idle":"2023-01-01T06:39:28.063225Z","shell.execute_reply":"2023-01-01T06:39:28.062144Z","shell.execute_reply.started":"2023-01-01T06:39:28.056578Z"},"trusted":true},"outputs":[],"source":["# # Get the dependencies for the virutal display\n","# !apt-get install python-opengl -y\n","# !apt install xvfb -y\n","# !pip install pyvirtualdisplay\n","# !pip install https://github.com/pyglet/pyglet/archive/pyglet-1.5-maintenance.zip\n","# !apt-get install ffmpeg -y"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:28.066983Z","iopub.status.busy":"2023-01-01T06:39:28.066194Z","iopub.status.idle":"2023-01-01T06:39:28.172825Z","shell.execute_reply":"2023-01-01T06:39:28.171886Z","shell.execute_reply.started":"2023-01-01T06:39:28.066883Z"},"trusted":true},"outputs":[],"source":["# from pyvirtualdisplay import Display\n","# import gym\n","# from gym import wrappers\n","# from gym import envs\n","# import matplotlib.pyplot as plt\n","\n","# # Setup the virutal display\n","# display = Display(visible=0,size=(600,600))\n","# display.start()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:28.176658Z","iopub.status.busy":"2023-01-01T06:39:28.176111Z","iopub.status.idle":"2023-01-01T06:39:30.706138Z","shell.execute_reply":"2023-01-01T06:39:30.703993Z","shell.execute_reply.started":"2023-01-01T06:39:28.176606Z"},"id":"1ujmj28vNyJJ","outputId":"2e3b2b58-b827-4c40-b933-d8f8477306f0","trusted":true},"outputs":[],"source":["# import retro for retro games (Street Fighter)\n","import retro\n","import retrowrapper\n","# import time to slow down the game\n","import time\n","import os \n","\n","# After downloading the ROM for Street Fighter, we used this command in the roms folder to connect it with our gym retro environment (python -m retro.import .)\n","!python -m retro.import .\n","# import the ROM for Street Fighter\n","gamename = \"StreetFighterIISpecialChampionEdition-Genesis\"\n","# env = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis')\n","env = retrowrapper.RetroWrapper(gamename, use_restricted_actions=retro.Actions.FILTERED)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:30.709853Z","iopub.status.busy":"2023-01-01T06:39:30.709328Z","iopub.status.idle":"2023-01-01T06:39:30.717801Z","shell.execute_reply":"2023-01-01T06:39:30.716455Z","shell.execute_reply.started":"2023-01-01T06:39:30.709798Z"},"trusted":true},"outputs":[],"source":["# # wrap the environment in monitor for rendering the training\n","# monitor_dir = os.getcwd()\n","# env = wrappers.Monitor(env,monitor_dir,video_callable=lambda ep_id: ep_id%1000 == 0,force=True)"]},{"cell_type":"markdown","metadata":{"id":"wJ8B-ALCNyJK"},"source":["### Figure out the observation and action space of the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:30.720631Z","iopub.status.busy":"2023-01-01T06:39:30.720059Z","iopub.status.idle":"2023-01-01T06:39:30.755434Z","shell.execute_reply":"2023-01-01T06:39:30.753807Z","shell.execute_reply.started":"2023-01-01T06:39:30.720580Z"},"id":"gCA1FhsuNyJK","outputId":"5fd48ab6-2d32-42f5-9c3d-27b01220f259","trusted":true},"outputs":[],"source":["env.observation_space"]},{"cell_type":"markdown","metadata":{"id":"U7625b68NyJL"},"source":["This most likely tells us that each observation is an image of height 200, width of 256, and 3 channels of RGB"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:30.758547Z","iopub.status.busy":"2023-01-01T06:39:30.757614Z","iopub.status.idle":"2023-01-01T06:39:30.770113Z","shell.execute_reply":"2023-01-01T06:39:30.768722Z","shell.execute_reply.started":"2023-01-01T06:39:30.758494Z"},"id":"u1CaRTVoNyJL","outputId":"80417e14-9210-430d-ef1e-7ae36206c631","trusted":true},"outputs":[],"source":["env.action_space\n","env.action_space.sample()"]},{"cell_type":"markdown","metadata":{"id":"se5WC-n-NyJL"},"source":["This means that we have a one-hot-encoded vector of length 12 to represent our action space. This means that we have 2^12 possible actions!"]},{"cell_type":"markdown","metadata":{"id":"MVSss2qPNyJM"},"source":["# Preprocess the environment\n","\n","### Agenda:\n","- Shrink the images so we have less pixels\n","- Calculate the frame delta (to understand movement and change within the game)\n","- Filter the action \n","- Set the reward function to the score of the game"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:30.773194Z","iopub.status.busy":"2023-01-01T06:39:30.772207Z","iopub.status.idle":"2023-01-01T06:39:30.781067Z","shell.execute_reply":"2023-01-01T06:39:30.779584Z","shell.execute_reply.started":"2023-01-01T06:39:30.773138Z"},"id":"ykgbyaDxNyJM","trusted":true},"outputs":[],"source":["# import the environment base class\n","from gym import Env\n","\n","# import opencv to process the image\n","import cv2\n","# import numpy to work calculate the frame delta\n","import numpy as np\n","# import the space shapes for our environment\n","from gym.spaces import MultiBinary, Box\n","# import matplotlib to plot the image\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:30.784405Z","iopub.status.busy":"2023-01-01T06:39:30.783402Z","iopub.status.idle":"2023-01-01T06:39:30.806446Z","shell.execute_reply":"2023-01-01T06:39:30.805013Z","shell.execute_reply.started":"2023-01-01T06:39:30.784334Z"},"id":"pod_22oJNyJM","trusted":true},"outputs":[],"source":["# Create custom environment\n","class StreetFighter(Env):\n","    def __init__(self):\n","        super().__init__()\n","        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n","        self.action_space = MultiBinary(12)\n","        # startup an instance of the game\n","        gamename = 'StreetFighterIISpecialChampionEdition-Genesis'\n","        self.game = retrowrapper.RetroWrapper(gamename, use_restricted_actions=retro.Actions.FILTERED)\n","        self.health = 176\n","        self.enemy_health = 176\n","    \n","    def step(self, action):\n","        obs, reward, done, info = self.game.step(action)\n","        obs = self.preprocess(obs)\n","        \n","        # Preprocess frame from game\n","        frame_delta = obs \n","#         - self.previous_frame\n","#         self.previous_frame = obs \n","        \n","        # Shape reward\n","        # find the difference in previous health and current health\n","        # finally discourage if our health went down negatively \n","        defense_reward = self.health - info['health']\n","        attack_reward = self.enemy_health - info['enemy_health']\n","        # score_reward = info['score'] - self.score \n","        # if our attack or defense reward are negative then don't account into reward function\n","        # this is because we don't want to reward when the round resets\n","        if attack_reward < 0:\n","            # NOTE that we are giving a huge positive reward for winning the round (this should incentivize the agent to finish the rounds faster)\n","            attack_reward = 150\n","            defense_reward = 0\n","            self.enemy_health = 176\n","            self.health = 176\n","        elif defense_reward <0:\n","            attack_reward = 0\n","            defense_reward = 0\n","            self.enemy_health = 176\n","            self.health = 176\n","        else:  \n","            # update the health variables with the new value if it wasn't negative \n","            self.enemy_health = info['enemy_health']\n","            self.health = info['health']\n","        # NOTE that the defense reward is a negative reward\n","        reward = attack_reward + (-1.0 * defense_reward)\n","\n","        return frame_delta, reward, done, info \n","\n","    def render(self, *args, **kwargs): \n","        self.game.render()\n","    \n","    def reset(self):\n","        self.previous_frame = np.zeros(self.game.observation_space.shape)\n","        \n","        # Frame delta\n","        obs = self.game.reset()\n","        obs = self.preprocess(obs)\n","        self.previous_frame = obs\n","        \n","        # Create initial variables\n","        # self.score = 0\n","        self.health = 176\n","        self.enemy_health = 176 \n","\n","        return obs\n","    \n","    def preprocess(self, observation): \n","        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n","        resize = cv2.resize(gray, (84,84), interpolation=cv2.INTER_CUBIC)\n","        state = np.reshape(resize, (84,84,1))\n","        return state\n","    \n","    def close(self): \n","        self.game.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:30.814763Z","iopub.status.busy":"2023-01-01T06:39:30.813882Z","iopub.status.idle":"2023-01-01T06:39:30.820405Z","shell.execute_reply":"2023-01-01T06:39:30.819357Z","shell.execute_reply.started":"2023-01-01T06:39:30.814710Z"},"id":"3U66l1fzNyJN","trusted":true},"outputs":[],"source":["# # Setup a game loop to see what the game looks like (testing)\n","# obs = env.reset()\n","# done = False\n","# # we are choosing to only play one game\n","# for game in range(1):\n","#     while not done:\n","#         if done:\n","#             obs = env.reset()\n","#         env.render()\n","#         action = env.action_space.sample()\n","#         obs, reward, done, info = env.step(action)\n","#         if reward > 0:\n","#             print(reward)"]},{"cell_type":"markdown","metadata":{"id":"vQ3JCDOuNyJO"},"source":["## Tune hyperparameters with Optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:30.822486Z","iopub.status.busy":"2023-01-01T06:39:30.822048Z","iopub.status.idle":"2023-01-01T06:39:33.158638Z","shell.execute_reply":"2023-01-01T06:39:33.157414Z","shell.execute_reply.started":"2023-01-01T06:39:30.822446Z"},"id":"2tSbGA4iNyJO","trusted":true},"outputs":[],"source":["import optuna \n","from stable_baselines3 import PPO\n","# useful for evaluting the current policy during our hyperparameter tuning\n","from stable_baselines3.common.evaluation import evaluate_policy\n","# import Monitor for logging\n","from stable_baselines3.common.monitor import Monitor\n","# import DummyVecEnv for vectorizing our environment and frame stacking\n","from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:33.162612Z","iopub.status.busy":"2023-01-01T06:39:33.161365Z","iopub.status.idle":"2023-01-01T06:39:33.171225Z","shell.execute_reply":"2023-01-01T06:39:33.166839Z","shell.execute_reply.started":"2023-01-01T06:39:33.162561Z"},"id":"yNfNJqCaNyJO","trusted":true},"outputs":[],"source":["LOG_DIR = \"./logs/\"\n","OPT_DIR = \"./opt/\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:33.173128Z","iopub.status.busy":"2023-01-01T06:39:33.172661Z","iopub.status.idle":"2023-01-01T06:39:33.182761Z","shell.execute_reply":"2023-01-01T06:39:33.181601Z","shell.execute_reply.started":"2023-01-01T06:39:33.173075Z"},"id":"HVfaL4j4NyJO","trusted":true},"outputs":[],"source":["# Function to return test hyperparameters\n","def optimize_ppo(trial):\n","    return {\n","        \"n_steps\": trial.suggest_int(\"n_steps\", 2048, 8192),\n","        \"gamma\": trial.suggest_loguniform(\"gamma\", 0.8, 0.9999),\n","        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-4),\n","        \"clip_range\": trial.suggest_uniform(\"clip_range\", 0.1, 0.4),\n","        \"gae_lambda\": trial.suggest_uniform(\"gae_lambda\", 0.8, 0.99),\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:33.185628Z","iopub.status.busy":"2023-01-01T06:39:33.184545Z","iopub.status.idle":"2023-01-01T06:39:33.211579Z","shell.execute_reply":"2023-01-01T06:39:33.210301Z","shell.execute_reply.started":"2023-01-01T06:39:33.185578Z"},"id":"F15gAb2QSsGZ","trusted":true},"outputs":[],"source":["env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:33.213868Z","iopub.status.busy":"2023-01-01T06:39:33.213431Z","iopub.status.idle":"2023-01-01T06:39:33.225529Z","shell.execute_reply":"2023-01-01T06:39:33.224280Z","shell.execute_reply.started":"2023-01-01T06:39:33.213830Z"},"id":"bdO9ZVwlNyJO","trusted":true},"outputs":[],"source":["# Setup the training loop and return the mean reward\n","total_steps = 100000\n","def train_ppo(trial):\n","    try:\n","        # setup the hyperparameters\n","        hyperparams = optimize_ppo(trial)\n","        # setup the environment\n","        env = StreetFighter()\n","        # setup the monitor (this is important since we are vectorizing the environment, because this allows us \n","        # to get the mean episode reward and mean episode length)\n","        env = Monitor(env, LOG_DIR)\n","        # setup the vectorized environment\n","        env = DummyVecEnv([lambda: env])\n","        # setup the frame stacking\n","        env = VecFrameStack(env, n_stack=4, channels_order='last')\n","        # setup the model\n","        model = PPO(\"CnnPolicy\", env, verbose=0, tensorboard_log=LOG_DIR, **hyperparams)\n","        # train the model\n","        model.learn(total_timesteps=total_steps)\n","        # evaluate the model\n","        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n","        # close the environment\n","        env.close()\n","\n","        # save the best model\n","        SAVE_PATH = os.path.join(OPT_DIR, \"trial_{}_best_model\".format(trial.number))\n","        model.save(SAVE_PATH)\n","\n","        return mean_reward\n","\n","    except Exception as e:\n","        print(e)\n","        return -1000"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:33.228059Z","iopub.status.busy":"2023-01-01T06:39:33.227373Z","iopub.status.idle":"2023-01-01T06:39:33.242797Z","shell.execute_reply":"2023-01-01T06:39:33.241526Z","shell.execute_reply.started":"2023-01-01T06:39:33.228020Z"},"id":"xOEjeIGZNyJP","outputId":"ca368eab-1b12-498e-c9b2-63862afbadcd","trusted":true},"outputs":[],"source":["# NOTE that since we used a positive reward function, we are maximizing the reward\n","# study = optuna.create_study(direction=\"maximize\")\n","# study.optimize(train_ppo, n_trials=5, n_jobs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:33.245688Z","iopub.status.busy":"2023-01-01T06:39:33.244822Z","iopub.status.idle":"2023-01-01T06:39:33.253807Z","shell.execute_reply":"2023-01-01T06:39:33.252763Z","shell.execute_reply.started":"2023-01-01T06:39:33.245642Z"},"id":"8RGwHB0mNyJP","trusted":true},"outputs":[],"source":["# best_model = PPO.load(os.path.join(OPT_DIR, \"trial_{}_best_model\".format(study.best_trial.number)))"]},{"cell_type":"markdown","metadata":{"id":"mlrTn8ZMNyJP"},"source":["# Setup Callback"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:33.256535Z","iopub.status.busy":"2023-01-01T06:39:33.255780Z","iopub.status.idle":"2023-01-01T06:39:33.264238Z","shell.execute_reply":"2023-01-01T06:39:33.263323Z","shell.execute_reply.started":"2023-01-01T06:39:33.256488Z"},"id":"up4-0bdNNyJP","trusted":true},"outputs":[],"source":["from stable_baselines3.common.callbacks import BaseCallback"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:33.267583Z","iopub.status.busy":"2023-01-01T06:39:33.266602Z","iopub.status.idle":"2023-01-01T06:39:33.277167Z","shell.execute_reply":"2023-01-01T06:39:33.276270Z","shell.execute_reply.started":"2023-01-01T06:39:33.267537Z"},"id":"xYtAE4jsNyJQ","trusted":true},"outputs":[],"source":["class TrainAndLoggingCallback(BaseCallback):\n","\n","    def __init__(self, check_freq, save_path, verbose=1):\n","        super(TrainAndLoggingCallback, self).__init__(verbose)\n","        self.check_freq = check_freq\n","        self.save_path = save_path\n","\n","    def _init_callback(self):\n","        if self.save_path is not None:\n","            os.makedirs(self.save_path, exist_ok=True)\n","\n","    def _on_step(self):\n","        if self.n_calls % self.check_freq == 0:\n","            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n","            self.model.save(model_path)\n","\n","        return True"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:33.279005Z","iopub.status.busy":"2023-01-01T06:39:33.278670Z","iopub.status.idle":"2023-01-01T06:39:33.293289Z","shell.execute_reply":"2023-01-01T06:39:33.291958Z","shell.execute_reply.started":"2023-01-01T06:39:33.278973Z"},"id":"mInrcbCsNyJQ","trusted":true},"outputs":[],"source":["CHECKPOINT_DIR = \"./train/\"\n","callback = TrainAndLoggingCallback(check_freq=250000, save_path=CHECKPOINT_DIR)"]},{"cell_type":"markdown","metadata":{"id":"ZwwB16QmNyJQ"},"source":["# Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:33.295828Z","iopub.status.busy":"2023-01-01T06:39:33.295396Z","iopub.status.idle":"2023-01-01T06:39:33.509217Z","shell.execute_reply":"2023-01-01T06:39:33.506837Z","shell.execute_reply.started":"2023-01-01T06:39:33.295793Z"},"id":"j8MVdSRANyJQ","trusted":true},"outputs":[],"source":["env.close()\n","# Recreate the environment\n","env = StreetFighter()\n","# setup the monitor (this is important since we are vectorizing the environment, because this allows us\n","# to get the mean episode reward and mean episode length)\n","env = Monitor(env, LOG_DIR)\n","# setup the vectorized environment\n","env = DummyVecEnv([lambda: env])\n","# setup the frame stacking\n","env = VecFrameStack(env, n_stack=4, channels_order='last')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:39:33.514413Z","iopub.status.busy":"2023-01-01T06:39:33.512814Z","iopub.status.idle":"2023-01-01T06:39:33.523962Z","shell.execute_reply":"2023-01-01T06:39:33.522582Z","shell.execute_reply.started":"2023-01-01T06:39:33.514331Z"},"trusted":true},"outputs":[],"source":["# Automatically choose the params n_steps to be the nearest factor of 64\n","# factored_steps = round(study.best_trial.params[\"n_steps\"] / 64) * 64"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-01T06:40:51.676475Z","iopub.status.busy":"2023-01-01T06:40:51.675971Z","iopub.status.idle":"2023-01-01T06:42:35.978980Z","shell.execute_reply":"2023-01-01T06:42:35.977414Z","shell.execute_reply.started":"2023-01-01T06:40:51.676428Z"},"id":"g-tfme6MNyJR","outputId":"ff4624c4-dc2f-43c6-d1a0-3837ff5231b0","trusted":true},"outputs":[],"source":["# update the current model params for the factoring\n","# model_params = study.best_trial.params\n","# model_params['n_steps'] = factored_steps\n","model_params = {'n_steps': 2560, 'gamma': 0.906, 'learning_rate': 2e-07, 'clip_range': 0.369, 'gae_lambda': 0.891}\n","model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n","\n","# update the model based on the previous trainings\n","# recreate the zip file for the best model so far\n","import shutil\n","model_name = \"best_model_nodelta_1\"\n","shutil.make_archive(\"best_model\", 'zip', f\"/kaggle/input/street-fighter/{model_name}\")\n","# load the model\n","model = PPO.load(\"/kaggle/working/best_model\", env)\n","\n","model.learn(total_timesteps=10000000, callback=callback)\n","env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-01T06:40:47.587025Z","iopub.status.idle":"2023-01-01T06:40:47.588506Z","shell.execute_reply":"2023-01-01T06:40:47.588253Z","shell.execute_reply.started":"2023-01-01T06:40:47.588226Z"},"trusted":true},"outputs":[],"source":["env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-01T06:40:47.589874Z","iopub.status.idle":"2023-01-01T06:40:47.590289Z","shell.execute_reply":"2023-01-01T06:40:47.590108Z","shell.execute_reply.started":"2023-01-01T06:40:47.590089Z"},"id":"86X4cNoIafZ3","trusted":true},"outputs":[],"source":["env = StreetFighter()\n","env = Monitor(env, LOG_DIR)\n","env = DummyVecEnv([lambda: env])\n","env = VecFrameStack(env, 4, channels_order='last')"]},{"cell_type":"markdown","metadata":{"id":"-83XlWLxNyJR"},"source":["# Test the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-01T06:40:47.591717Z","iopub.status.idle":"2023-01-01T06:40:47.592104Z","shell.execute_reply":"2023-01-01T06:40:47.591931Z","shell.execute_reply.started":"2023-01-01T06:40:47.591914Z"},"trusted":true},"outputs":[],"source":["# # display the training\n","# from IPython.display import HTML\n","# from base64 import b64encode\n","\n","# video = [v for v in os.listdir('./') if 'mp4' in v]\n","# video.sort()\n","# print(len(video))\n","# # print(video[:26])\n","# vid_1 = open(video[0],'rb').read()\n","# data_url_1 = \"data:video/mp4;base64,\" + b64encode(vid_1).decode()\n","# HTML(\"\"\"\n","# <video width=600 height=600 controls>\n","#       <source src=\"%s\" type=\"video/mp4\">\n","# </video>\n","# \"\"\" % data_url_1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-01T06:40:47.594618Z","iopub.status.idle":"2023-01-01T06:40:47.595053Z","shell.execute_reply":"2023-01-01T06:40:47.594863Z","shell.execute_reply.started":"2023-01-01T06:40:47.594843Z"},"trusted":true},"outputs":[],"source":["# # Create an HTML video frame for it if the previous video frame didn't work\n","# vid_2 = open(video[-1],'rb').read()\n","# data_url_2 = \"data:video/mp4;base64,\" + b64encode(vid_2).decode()\n","# HTML(\"\"\"\n","# <video width=600 height=600 controls>\n","#       <source src=\"%s\" type=\"video/mp4\">\n","# </video>\n","# \"\"\" % data_url_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-01T06:40:47.596420Z","iopub.status.idle":"2023-01-01T06:40:47.596852Z","shell.execute_reply":"2023-01-01T06:40:47.596656Z","shell.execute_reply.started":"2023-01-01T06:40:47.596630Z"},"id":"2CTOPrNMNyJS","trusted":true},"outputs":[],"source":["# import time\n","\n","# # code to render the agent's progress and log the rewards\n","# for episode in range(1): \n","#     obs = env.reset()\n","#     done = False\n","#     total_reward = 0\n","#     while not done: \n","#         action, _ = model.predict(obs)\n","#         obs, reward, done, info = env.step(action)\n","#         env.render()\n","#         time.sleep(0.01)\n","#         total_reward += reward\n","#     print('Total Reward for episode {} is {}'.format(total_reward, episode))\n","#     time.sleep(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"street_fighter","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11 (default, Aug  6 2021, 08:56:27) \n[Clang 10.0.0 ]"},"vscode":{"interpreter":{"hash":"9c3846ce54bed1895ba0d2c90c8f55d61bce045021babe443836d90a30036b65"}}},"nbformat":4,"nbformat_minor":4}
