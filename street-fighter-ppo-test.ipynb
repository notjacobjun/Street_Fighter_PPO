{"cells":[{"cell_type":"markdown","metadata":{"id":"Tt5FpWrgNyJF"},"source":["## Setup the StreetFighter environment"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-29T12:25:52.624716Z","iopub.status.busy":"2022-12-29T12:25:52.623583Z","iopub.status.idle":"2022-12-29T12:25:55.512157Z","shell.execute_reply":"2022-12-29T12:25:55.510610Z","shell.execute_reply.started":"2022-12-29T12:25:52.624647Z"},"id":"1ujmj28vNyJJ","outputId":"2e3b2b58-b827-4c40-b933-d8f8477306f0","trusted":true},"outputs":[],"source":["# import retro for retro games (Street Fighter)\n","import retro\n","import retrowrapper\n","# use the time module to slow down the game if needed when viewing \n","import time\n","import os \n","\n","# After downloading the ROM for Street Fighter, we used this command in the roms folder to connect it with our gym retro environment (python -m retro.import .)\n","# !python -m retro.import ../input/street-fighter-rom\n","# import the ROM for Street Fighter\n","gamename = \"StreetFighterIISpecialChampionEdition-Genesis\"\n","env = retrowrapper.RetroWrapper(gamename, use_restricted_actions=retro.Actions.FILTERED)"]},{"cell_type":"markdown","metadata":{"id":"wJ8B-ALCNyJK"},"source":["### Figure out the observation and action space of the environment"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-29T12:25:55.516547Z","iopub.status.busy":"2022-12-29T12:25:55.516026Z","iopub.status.idle":"2022-12-29T12:25:55.534801Z","shell.execute_reply":"2022-12-29T12:25:55.532510Z","shell.execute_reply.started":"2022-12-29T12:25:55.516497Z"},"id":"gCA1FhsuNyJK","outputId":"5fd48ab6-2d32-42f5-9c3d-27b01220f259","trusted":true},"outputs":[{"data":{"text/plain":["Box([[[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]\n","\n"," [[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]\n","\n"," [[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]\n","\n"," ...\n","\n"," [[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]\n","\n"," [[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]\n","\n"," [[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]], [[[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]\n","\n"," [[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]\n","\n"," [[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]\n","\n"," ...\n","\n"," [[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]\n","\n"," [[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]\n","\n"," [[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]], (200, 256, 3), uint8)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["env.observation_space"]},{"cell_type":"markdown","metadata":{"id":"U7625b68NyJL"},"source":["This most likely tells us that each observation is an image of height 200, width of 256, and 3 channels of RGB"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-29T12:25:55.538687Z","iopub.status.busy":"2022-12-29T12:25:55.537613Z","iopub.status.idle":"2022-12-29T12:25:55.559234Z","shell.execute_reply":"2022-12-29T12:25:55.557994Z","shell.execute_reply.started":"2022-12-29T12:25:55.538605Z"},"id":"u1CaRTVoNyJL","outputId":"80417e14-9210-430d-ef1e-7ae36206c631","trusted":true},"outputs":[{"data":{"text/plain":["array([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0], dtype=int8)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["env.action_space\n","env.action_space.sample()"]},{"cell_type":"markdown","metadata":{"id":"se5WC-n-NyJL"},"source":["This means that we have a one-hot-encoded vector of length 12 to represent our action space. This means that we have 2^12 possible actions!"]},{"cell_type":"markdown","metadata":{"id":"MVSss2qPNyJM"},"source":["# Preprocess the environment\n","\n","### Agenda:\n","- Shrink the images so we have less pixels\n","- Calculate the frame delta (to understand movement and change within the game)\n","- Filter the action \n","- Set the reward function to the score of the game"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:55.561691Z","iopub.status.busy":"2022-12-29T12:25:55.560538Z","iopub.status.idle":"2022-12-29T12:25:55.569507Z","shell.execute_reply":"2022-12-29T12:25:55.567931Z","shell.execute_reply.started":"2022-12-29T12:25:55.561652Z"},"id":"ykgbyaDxNyJM","trusted":true},"outputs":[],"source":["# import the environment base class\n","from gym import Env\n","\n","# import opencv to process the image\n","import cv2\n","# import numpy to work calculate the frame delta\n","import numpy as np\n","# import the space shapes for our environment\n","from gym.spaces import MultiBinary, Box\n","# import matplotlib to plot the image\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:55.572282Z","iopub.status.busy":"2022-12-29T12:25:55.570976Z","iopub.status.idle":"2022-12-29T12:25:55.585952Z","shell.execute_reply":"2022-12-29T12:25:55.584500Z","shell.execute_reply.started":"2022-12-29T12:25:55.572244Z"},"id":"pod_22oJNyJM","trusted":true},"outputs":[],"source":["# Create custom environment\n","class StreetFighter(Env):\n","    def __init__(self):\n","        super().__init__()\n","        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n","        self.action_space = MultiBinary(12)\n","        # startup an instance of the game\n","        gamename = 'StreetFighterIISpecialChampionEdition-Genesis'\n","        self.game = retrowrapper.RetroWrapper(gamename, use_restricted_actions=retro.Actions.FILTERED)\n","    \n","    def step(self, action):\n","        # take a step (using the base environment)\n","        obs, reward, done, info = self.game.step(action)\n","        # preprocess the observation\n","        obs = self.preprocess(obs)\n","\n","        # calculate the frame delta\n","        frame_delta = obs - self.previous_frame\n","        self.previous_frame = obs\n","\n","        # calculate the score delta and reshape the reward function based on the score in the environment\n","        reward = info['score'] - self.score\n","        self.score = info['score']\n","\n","        return frame_delta, reward, done, info\n","\n","    def reset(self):\n","        obs = self.game.reset()\n","        # preprocess the image\n","        obs = self.preprocess(obs)\n","        # initialize the previous_frame value with the first frame\n","        self.previous_frame = obs\n","        # create a default value for the score delta\n","        self.score = 0\n","        return obs\n","    \n","    def preprocess(self, observation):\n","        # Grayscaling \n","        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n","        # resize the image\n","        resize = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)\n","        # add the channels value \n","        channels = np.reshape(resize, (84, 84, 1))\n","\n","        return channels\n","        \n","    def render(self, *args, **kwargs):\n","        self.game.render()\n","\n","    def close(self):\n","        self.game.close()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:55.588725Z","iopub.status.busy":"2022-12-29T12:25:55.588150Z","iopub.status.idle":"2022-12-29T12:25:55.604973Z","shell.execute_reply":"2022-12-29T12:25:55.603300Z","shell.execute_reply.started":"2022-12-29T12:25:55.588694Z"},"id":"3U66l1fzNyJN","trusted":true},"outputs":[],"source":["# # Setup a game loop to see what the game looks like (testing)\n","# obs = env.reset()\n","# done = False\n","# # we are choosing to only play one game\n","# for game in range(1):\n","#     while not done:\n","#         if done:\n","#             obs = env.reset()\n","#         env.render()\n","#         action = env.action_space.sample()\n","#         obs, reward, done, info = env.step(action)\n","#         if reward > 0:\n","#             print(reward)"]},{"cell_type":"markdown","metadata":{"id":"vQ3JCDOuNyJO"},"source":["## Tune hyperparameters with Optuna"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:55.607469Z","iopub.status.busy":"2022-12-29T12:25:55.606970Z","iopub.status.idle":"2022-12-29T12:25:58.349991Z","shell.execute_reply":"2022-12-29T12:25:58.348680Z","shell.execute_reply.started":"2022-12-29T12:25:55.607419Z"},"id":"2tSbGA4iNyJO","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/jacobjun/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import optuna \n","from stable_baselines3 import PPO\n","# useful for evaluting the current policy during our hyperparameter tuning\n","from stable_baselines3.common.evaluation import evaluate_policy\n","# import Monitor for logging\n","from stable_baselines3.common.monitor import Monitor\n","# import DummyVecEnv for vectorizing our environment and frame stacking\n","from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.355179Z","iopub.status.busy":"2022-12-29T12:25:58.354624Z","iopub.status.idle":"2022-12-29T12:25:58.361767Z","shell.execute_reply":"2022-12-29T12:25:58.360035Z","shell.execute_reply.started":"2022-12-29T12:25:58.355147Z"},"id":"yNfNJqCaNyJO","trusted":true},"outputs":[],"source":["LOG_DIR = \"./logs/\"\n","OPT_DIR = \"./opt/\""]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.364177Z","iopub.status.busy":"2022-12-29T12:25:58.363690Z","iopub.status.idle":"2022-12-29T12:25:58.376322Z","shell.execute_reply":"2022-12-29T12:25:58.374988Z","shell.execute_reply.started":"2022-12-29T12:25:58.364132Z"},"id":"HVfaL4j4NyJO","trusted":true},"outputs":[],"source":["# Function to return test hyperparameters\n","def optimize_ppo(trial):\n","    return {\n","        \"n_steps\": trial.suggest_int(\"n_steps\", 2048, 8192),\n","        \"gamma\": trial.suggest_loguniform(\"gamma\", 0.8, 0.9999),\n","        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-4),\n","        \"clip_range\": trial.suggest_uniform(\"clip_range\", 0.1, 0.4),\n","        \"gae_lambda\": trial.suggest_uniform(\"gae_lambda\", 0.8, 0.99),\n","    }"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.378380Z","iopub.status.busy":"2022-12-29T12:25:58.378049Z","iopub.status.idle":"2022-12-29T12:25:58.396323Z","shell.execute_reply":"2022-12-29T12:25:58.395245Z","shell.execute_reply.started":"2022-12-29T12:25:58.378352Z"},"id":"F15gAb2QSsGZ","trusted":true},"outputs":[],"source":["env.close()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.398124Z","iopub.status.busy":"2022-12-29T12:25:58.397776Z","iopub.status.idle":"2022-12-29T12:25:58.411452Z","shell.execute_reply":"2022-12-29T12:25:58.410299Z","shell.execute_reply.started":"2022-12-29T12:25:58.398093Z"},"id":"bdO9ZVwlNyJO","trusted":true},"outputs":[],"source":["# Setup the training loop and return the mean reward\n","total_steps = 100000\n","def train_ppo(trial):\n","    try:\n","        # setup the hyperparameters\n","        hyperparams = optimize_ppo(trial)\n","        # setup the environment\n","        env = StreetFighter()\n","        # setup the monitor (this is important since we are vectorizing the environment, because this allows us \n","        # to get the mean episode reward and mean episode length)\n","        env = Monitor(env, LOG_DIR)\n","        # setup the vectorized environment\n","        env = DummyVecEnv([lambda: env])\n","        # setup the frame stacking\n","        env = VecFrameStack(env, n_stack=4, channels_order='last')\n","        # setup the model\n","        model = PPO(\"CnnPolicy\", env, verbose=0, tensorboard_log=LOG_DIR, **hyperparams)\n","        # train the model\n","        model.learn(total_timesteps=total_steps)\n","        # evaluate the model\n","        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n","        # close the environment\n","        env.close()\n","\n","        # save the best model\n","        SAVE_PATH = os.path.join(OPT_DIR, \"trial_{}_best_model\".format(trial.number))\n","        model.save(SAVE_PATH)\n","\n","        return mean_reward\n","\n","    except Exception as e:\n","        return -1000"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-29T12:25:58.414157Z","iopub.status.busy":"2022-12-29T12:25:58.413261Z","iopub.status.idle":"2022-12-29T12:25:58.430347Z","shell.execute_reply":"2022-12-29T12:25:58.428961Z","shell.execute_reply.started":"2022-12-29T12:25:58.414101Z"},"id":"xOEjeIGZNyJP","outputId":"ca368eab-1b12-498e-c9b2-63862afbadcd","trusted":true},"outputs":[],"source":["# NOTE that since we used a positive reward function, we are maximizing the reward\n","# study = optuna.create_study(direction=\"maximize\")\n","# study.optimize(train_ppo, n_trials=100, n_jobs=1)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.433129Z","iopub.status.busy":"2022-12-29T12:25:58.432128Z","iopub.status.idle":"2022-12-29T12:25:58.446316Z","shell.execute_reply":"2022-12-29T12:25:58.444890Z","shell.execute_reply.started":"2022-12-29T12:25:58.433082Z"},"id":"8RGwHB0mNyJP","trusted":true},"outputs":[],"source":["# best_model = PPO.load(os.path.join(OPT_DIR, \"trial_{}_best_model\".format(study.best_trial.number)))"]},{"cell_type":"markdown","metadata":{"id":"mlrTn8ZMNyJP"},"source":["# Setup Callback"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.448547Z","iopub.status.busy":"2022-12-29T12:25:58.447927Z","iopub.status.idle":"2022-12-29T12:25:58.458675Z","shell.execute_reply":"2022-12-29T12:25:58.457270Z","shell.execute_reply.started":"2022-12-29T12:25:58.448512Z"},"id":"up4-0bdNNyJP","trusted":true},"outputs":[],"source":["from stable_baselines3.common.callbacks import BaseCallback"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.462007Z","iopub.status.busy":"2022-12-29T12:25:58.460466Z","iopub.status.idle":"2022-12-29T12:25:58.476066Z","shell.execute_reply":"2022-12-29T12:25:58.474548Z","shell.execute_reply.started":"2022-12-29T12:25:58.461948Z"},"id":"xYtAE4jsNyJQ","trusted":true},"outputs":[],"source":["class TrainAndLoggingCallback(BaseCallback):\n","\n","    def __init__(self, check_freq, save_path, verbose=1):\n","        super(TrainAndLoggingCallback, self).__init__(verbose)\n","        self.check_freq = check_freq\n","        self.save_path = save_path\n","\n","    def _init_callback(self):\n","        if self.save_path is not None:\n","            os.makedirs(self.save_path, exist_ok=True)\n","\n","    def _on_step(self):\n","        if self.n_calls % self.check_freq == 0:\n","            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n","            self.model.save(model_path)\n","\n","        return True"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.478665Z","iopub.status.busy":"2022-12-29T12:25:58.477794Z","iopub.status.idle":"2022-12-29T12:25:58.504137Z","shell.execute_reply":"2022-12-29T12:25:58.501461Z","shell.execute_reply.started":"2022-12-29T12:25:58.478598Z"},"id":"mInrcbCsNyJQ","trusted":true},"outputs":[],"source":["CHECKPOINT_DIR = \"./train/\"\n","callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"]},{"cell_type":"markdown","metadata":{"id":"ZwwB16QmNyJQ"},"source":["# Train Model"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.507384Z","iopub.status.busy":"2022-12-29T12:25:58.506476Z","iopub.status.idle":"2022-12-29T12:25:58.775392Z","shell.execute_reply":"2022-12-29T12:25:58.774089Z","shell.execute_reply.started":"2022-12-29T12:25:58.507304Z"},"id":"j8MVdSRANyJQ","trusted":true},"outputs":[],"source":["env.close()\n","# Recreate the environment\n","env = StreetFighter()\n","# setup the monitor (this is important since we are vectorizing the environment, because this allows us\n","# to get the mean episode reward and mean episode length)\n","env = Monitor(env, LOG_DIR)\n","# setup the vectorized environment\n","env = DummyVecEnv([lambda: env])\n","# setup the frame stacking\n","env = VecFrameStack(env, n_stack=4, channels_order='last')"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-29T12:25:58.778143Z","iopub.status.busy":"2022-12-29T12:25:58.777270Z","iopub.status.idle":"2022-12-29T12:25:59.074645Z","shell.execute_reply":"2022-12-29T12:25:59.073618Z","shell.execute_reply.started":"2022-12-29T12:25:58.778090Z"},"id":"g-tfme6MNyJR","outputId":"ff4624c4-dc2f-43c6-d1a0-3837ff5231b0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu device\n","Wrapping the env in a VecTransposeImage.\n"]}],"source":["# code that we used to originally train the model\n","# We got these model params from the hyperparameter optimization trials\n","model_params = {'n_steps': 2570.949, 'gamma': 0.906, 'learning_rate': 2e-07, 'clip_range': 0.369, 'gae_lambda': 0.891}\n","model_params['n_steps'] = 40 * 64 # based on optuna study from above (rounding to nearest factor of 64)\n","\n","model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n","# model.learn(total_timesteps=5000000, callback=callback)\n","env.close()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["{'n_steps': 2560,\n"," 'gamma': 0.906,\n"," 'learning_rate': 2e-07,\n"," 'clip_range': 0.369,\n"," 'gae_lambda': 0.891}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model_params"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:54:34.581513Z","iopub.status.busy":"2022-12-29T12:54:34.580918Z","iopub.status.idle":"2022-12-29T12:54:36.625409Z","shell.execute_reply":"2022-12-29T12:54:36.623970Z","shell.execute_reply.started":"2022-12-29T12:54:34.581458Z"},"trusted":true},"outputs":[],"source":["# # recreate the zip file for the best model so far\n","# import shutil\n","# shutil.make_archive(\"best_model\", 'zip', \"/kaggle/input/street-fighter-rom/best_model_5460000\")\n","# # load the model \n","# model = PPO.load(\"/kaggle/working/best_model\")"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"execution":{"iopub.execute_input":"2022-12-29T12:54:30.947001Z","iopub.status.busy":"2022-12-29T12:54:30.945566Z","iopub.status.idle":"2022-12-29T12:54:30.953077Z","shell.execute_reply":"2022-12-29T12:54:30.951337Z","shell.execute_reply.started":"2022-12-29T12:54:30.946950Z"},"id":"SYTS0HZpYoxG","outputId":"4cdac382-4e91-414e-db4c-bd58066e2d2f","trusted":true},"outputs":[],"source":["# Load the model from the provided training (to save compute resources)\n","custom_objects = {\n","        \"learning_rate\": 2e-07,\n","        \"lr_schedule\": lambda _: 0.0,\n","        \"clip_range\": lambda _: 0.369,\n","    }\n","model_version = \"best_model_optuna_1\"\n","model = PPO.load(f'./train/{model_version}.zip', custom_objects=custom_objects)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:54:55.661534Z","iopub.status.busy":"2022-12-29T12:54:55.661078Z","iopub.status.idle":"2022-12-29T12:54:55.954967Z","shell.execute_reply":"2022-12-29T12:54:55.953601Z","shell.execute_reply.started":"2022-12-29T12:54:55.661500Z"},"id":"86X4cNoIafZ3","trusted":true},"outputs":[],"source":["env = StreetFighter()\n","env = Monitor(env, LOG_DIR)\n","env = DummyVecEnv([lambda: env])\n","env = VecFrameStack(env, 4, channels_order='last')"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:54:59.040311Z","iopub.status.busy":"2022-12-29T12:54:59.039819Z","iopub.status.idle":"2022-12-29T12:54:59.514549Z","shell.execute_reply":"2022-12-29T12:54:59.511869Z","shell.execute_reply.started":"2022-12-29T12:54:59.040273Z"},"id":"DAPPaMhxNyJR","trusted":true},"outputs":[],"source":["# mean_reward, _ = evaluate_policy(model, env, render=True, n_eval_episodes=1)\n","# print(mean_reward)\n","# env.close()"]},{"cell_type":"markdown","metadata":{"id":"-83XlWLxNyJR"},"source":["# Test the model"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.status.busy":"2022-12-29T12:25:59.276782Z","iopub.status.idle":"2022-12-29T12:25:59.277328Z","shell.execute_reply":"2022-12-29T12:25:59.277122Z","shell.execute_reply.started":"2022-12-29T12:25:59.277099Z"},"id":"2CTOPrNMNyJS","trusted":true},"outputs":[],"source":["# import time\n","\n","# # code to render the agent's progress and log the rewards\n","# for episode in range(1): \n","#     obs = env.reset()\n","#     done = False\n","#     total_reward = 0\n","#     while not done: \n","#         action, _ = model.predict(obs)\n","#         obs, reward, done, info = env.step(action)\n","#         env.render()\n","#         time.sleep(0.01)\n","#         total_reward += reward\n","#     print('Total Reward for episode {} is {}'.format(total_reward, episode))\n","#     time.sleep(2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Record a video of the current progress"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from stable_baselines3.common.vec_env import VecVideoRecorder\n","import imageio\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","import glob\n","import io\n","import base64\n","from gym.wrappers import Monitor\n","\n","video_folder = \"./logs/\"\n","video_length = 350\n","\n","def create_mp4(model, env):\n","    model.set_env(env)\n","    # Record the video starting at the first step\n","    env = VecVideoRecorder(env, video_folder,\n","                        record_video_trigger=lambda x: x == 0, video_length=video_length,\n","                        name_prefix=\"ppo-sf2{}\".format(gamename))\n","\n","    # update the model's env\n","    model.set_env(env)\n","    obs = model.env.reset()\n","    for _ in range(video_length + 1):\n","        action = model.predict(obs)\n","        obs, _, _, _ = model.env.step(action)\n","    # Save the video\n","    model.env.close()\n","\n","\n","def create_gif(model, env):\n","    model.set_env(env)\n","    images = []\n","    obs = model.env.reset()\n","    img = model.env.render()\n","    # TODO the current issue is that our render method returns None (which makes sense since the return value is optional)\n","    print(img)\n","    for i in range(350):\n","        images.append(img)\n","        action, _ = model.predict(obs)\n","        obs, _, _ ,_ = model.env.step(action)\n","        img = model.env.render()\n","        print(img)\n","\n","    imageio.mimsave(f'ppo_sf2_{model_version}.gif', [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=60)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Wrapping the env in a VecTransposeImage.\n","Wrapping the env in a VecTransposeImage.\n"]},{"name":"stderr","output_type":"stream","text":["Process Process-3:\n","Traceback (most recent call last):\n","  File \"/Users/jacobjun/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n","    self.run()\n","  File \"/Users/jacobjun/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/Users/jacobjun/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/retrowrapper.py\", line 37, in _retrocom\n","    result = result(*args, **kwargs)\n","  File \"/Users/jacobjun/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/retro/retro_env.py\", line 179, in step\n","    for p, ap in enumerate(self.action_to_array(a)):\n","  File \"/Users/jacobjun/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/retro/retro_env.py\", line 166, in action_to_array\n","    action |= int(ap[i]) << i\n","TypeError: only size-1 arrays can be converted to Python scalars\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_mp4(model, env)\n","Cell \u001b[0;32mIn[27], line 25\u001b[0m, in \u001b[0;36mcreate_mp4\u001b[0;34m(model, env)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(video_length \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     24\u001b[0m     action \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(obs)\n\u001b[0;32m---> 25\u001b[0m     obs, _, _, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     26\u001b[0m \u001b[39m# Save the video\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mclose()\n","File \u001b[0;32m~/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n","File \u001b[0;32m~/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[1;32m     97\u001b[0m     \u001b[39m# Transpose the terminal observations\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39mfor\u001b[39;00m idx, done \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dones):\n","File \u001b[0;32m~/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:88\u001b[0m, in \u001b[0;36mVecVideoRecorder.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 88\u001b[0m     obs, rews, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[1;32m     90\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_id \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     91\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecording:\n","File \u001b[0;32m~/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:48\u001b[0m, in \u001b[0;36mVecFrameStack.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     46\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]], np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mndarray, List[Dict[\u001b[39mstr\u001b[39m, Any]],]:\n\u001b[0;32m---> 48\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[1;32m     50\u001b[0m     observations, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstackedobs\u001b[39m.\u001b[39mupdate(observations, dones, infos)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m observations, rewards, dones, infos\n","File \u001b[0;32m~/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     45\u001b[0m         )\n\u001b[1;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n","File \u001b[0;32m~/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m done:\n","Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mStreetFighter.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     12\u001b[0m     \u001b[39m# take a step (using the base environment)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     14\u001b[0m     \u001b[39m# preprocess the observation\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(obs)\n","File \u001b[0;32m~/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/retrowrapper.py:125\u001b[0m, in \u001b[0;36mRetroWrapper.__getattr__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tx\u001b[39m.\u001b[39mput((attr, args, kwargs))\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rx\u001b[39m.\u001b[39;49mget()\n","File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/queues.py:97\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m block \u001b[39mand\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlock:\n\u001b[0;32m---> 97\u001b[0m         res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes()\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sem\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mif\u001b[39;00m maxlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m maxlength \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnegative maxlength\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes(maxlength)\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m buf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bad_message_length()\n","File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_recv_bytes\u001b[39m(\u001b[39mself\u001b[39m, maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    415\u001b[0m     size, \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m!i\u001b[39m\u001b[39m\"\u001b[39m, buf\u001b[39m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m size \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n","File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[39m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m remaining \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[39m=\u001b[39m read(handle, remaining)\n\u001b[1;32m    380\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["create_mp4(model, env)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"street_fighter","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"vscode":{"interpreter":{"hash":"9c3846ce54bed1895ba0d2c90c8f55d61bce045021babe443836d90a30036b65"}}},"nbformat":4,"nbformat_minor":4}
