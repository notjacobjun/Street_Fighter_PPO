{"cells":[{"cell_type":"markdown","metadata":{"id":"Tt5FpWrgNyJF"},"source":["## Setup the StreetFighter environment"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-29T12:25:52.624716Z","iopub.status.busy":"2022-12-29T12:25:52.623583Z","iopub.status.idle":"2022-12-29T12:25:55.512157Z","shell.execute_reply":"2022-12-29T12:25:55.510610Z","shell.execute_reply.started":"2022-12-29T12:25:52.624647Z"},"id":"1ujmj28vNyJJ","outputId":"2e3b2b58-b827-4c40-b933-d8f8477306f0","trusted":true},"outputs":[],"source":["# import retro for retro games (Street Fighter)\n","import retro\n","import retrowrapper\n","# use the time module to slow down the game if needed when viewing \n","import time\n","import os \n","\n","# After downloading the ROM for Street Fighter, we used this command in the roms folder to connect it with our gym retro environment (python -m retro.import .)\n","# !python -m retro.import ../input/street-fighter-rom\n","# import the ROM for Street Fighter\n","gamename = \"StreetFighterIISpecialChampionEdition-Genesis\"\n","env = retrowrapper.RetroWrapper(gamename, use_restricted_actions=retro.Actions.FILTERED)"]},{"cell_type":"markdown","metadata":{"id":"wJ8B-ALCNyJK"},"source":["### Figure out the observation and action space of the environment"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-29T12:25:55.516547Z","iopub.status.busy":"2022-12-29T12:25:55.516026Z","iopub.status.idle":"2022-12-29T12:25:55.534801Z","shell.execute_reply":"2022-12-29T12:25:55.532510Z","shell.execute_reply.started":"2022-12-29T12:25:55.516497Z"},"id":"gCA1FhsuNyJK","outputId":"5fd48ab6-2d32-42f5-9c3d-27b01220f259","trusted":true},"outputs":[{"data":{"text/plain":["Box([[[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]\n","\n"," [[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]\n","\n"," [[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]\n","\n"," ...\n","\n"," [[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]\n","\n"," [[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]\n","\n"," [[0 0 0]\n","  [0 0 0]\n","  [0 0 0]\n","  ...\n","  [0 0 0]\n","  [0 0 0]\n","  [0 0 0]]], [[[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]\n","\n"," [[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]\n","\n"," [[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]\n","\n"," ...\n","\n"," [[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]\n","\n"," [[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]\n","\n"," [[255 255 255]\n","  [255 255 255]\n","  [255 255 255]\n","  ...\n","  [255 255 255]\n","  [255 255 255]\n","  [255 255 255]]], (200, 256, 3), uint8)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["env.observation_space"]},{"cell_type":"markdown","metadata":{"id":"U7625b68NyJL"},"source":["This most likely tells us that each observation is an image of height 200, width of 256, and 3 channels of RGB"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-29T12:25:55.538687Z","iopub.status.busy":"2022-12-29T12:25:55.537613Z","iopub.status.idle":"2022-12-29T12:25:55.559234Z","shell.execute_reply":"2022-12-29T12:25:55.557994Z","shell.execute_reply.started":"2022-12-29T12:25:55.538605Z"},"id":"u1CaRTVoNyJL","outputId":"80417e14-9210-430d-ef1e-7ae36206c631","trusted":true},"outputs":[{"data":{"text/plain":["array([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int8)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["env.action_space\n","env.action_space.sample()"]},{"cell_type":"markdown","metadata":{"id":"se5WC-n-NyJL"},"source":["This means that we have a one-hot-encoded vector of length 12 to represent our action space. This means that we have 2^12 possible actions!"]},{"cell_type":"markdown","metadata":{"id":"MVSss2qPNyJM"},"source":["# Preprocess the environment\n","\n","### Agenda:\n","- Shrink the images so we have less pixels\n","- Calculate the frame delta (to understand movement and change within the game)\n","- Filter the action \n","- Set the reward function to the score of the game"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:55.561691Z","iopub.status.busy":"2022-12-29T12:25:55.560538Z","iopub.status.idle":"2022-12-29T12:25:55.569507Z","shell.execute_reply":"2022-12-29T12:25:55.567931Z","shell.execute_reply.started":"2022-12-29T12:25:55.561652Z"},"id":"ykgbyaDxNyJM","trusted":true},"outputs":[],"source":["# import the environment base class\n","from gym import Env\n","\n","# import opencv to process the image\n","import cv2\n","# import numpy to work calculate the frame delta\n","import numpy as np\n","# import the space shapes for our environment\n","from gym.spaces import MultiBinary, Box\n","# import matplotlib to plot the image\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:55.572282Z","iopub.status.busy":"2022-12-29T12:25:55.570976Z","iopub.status.idle":"2022-12-29T12:25:55.585952Z","shell.execute_reply":"2022-12-29T12:25:55.584500Z","shell.execute_reply.started":"2022-12-29T12:25:55.572244Z"},"id":"pod_22oJNyJM","trusted":true},"outputs":[],"source":["# Create custom environment\n","class StreetFighter(Env):\n","    def __init__(self):\n","        super().__init__()\n","        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n","        self.action_space = MultiBinary(12)\n","        # startup an instance of the game\n","        gamename = 'StreetFighterIISpecialChampionEdition-Genesis'\n","        self.game = retrowrapper.RetroWrapper(gamename, use_restricted_actions=retro.Actions.FILTERED)\n","    \n","    def step(self, action):\n","        # take a step (using the base environment)\n","        obs, reward, done, info = self.game.step(action)\n","        # preprocess the observation\n","        obs = self.preprocess(obs)\n","\n","        # calculate the frame delta\n","        frame_delta = obs - self.previous_frame\n","        self.previous_frame = obs\n","\n","        # calculate the score delta and reshape the reward function based on the score in the environment\n","        reward = info['score'] - self.score\n","        self.score = info['score']\n","\n","        return frame_delta, reward, done, info\n","\n","    def reset(self):\n","        obs = self.game.reset()\n","        # preprocess the image\n","        obs = self.preprocess(obs)\n","        # initialize the previous_frame value with the first frame\n","        self.previous_frame = obs\n","        # create a default value for the score delta\n","        self.score = 0\n","        return obs\n","    \n","    def preprocess(self, observation):\n","        # Grayscaling \n","        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n","        # resize the image\n","        resize = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)\n","        # add the channels value \n","        channels = np.reshape(resize, (84, 84, 1))\n","\n","        return channels\n","        \n","    def render(self, *args, **kwargs):\n","        self.game.render()\n","\n","    def close(self):\n","        self.game.close()"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:55.588725Z","iopub.status.busy":"2022-12-29T12:25:55.588150Z","iopub.status.idle":"2022-12-29T12:25:55.604973Z","shell.execute_reply":"2022-12-29T12:25:55.603300Z","shell.execute_reply.started":"2022-12-29T12:25:55.588694Z"},"id":"3U66l1fzNyJN","trusted":true},"outputs":[],"source":["# # Setup a game loop to see what the game looks like (testing)\n","# obs = env.reset()\n","# done = False\n","# # we are choosing to only play one game\n","# for game in range(1):\n","#     while not done:\n","#         if done:\n","#             obs = env.reset()\n","#         env.render()\n","#         action = env.action_space.sample()\n","#         obs, reward, done, info = env.step(action)\n","#         if reward > 0:\n","#             print(reward)"]},{"cell_type":"markdown","metadata":{"id":"vQ3JCDOuNyJO"},"source":["## Tune hyperparameters with Optuna"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:55.607469Z","iopub.status.busy":"2022-12-29T12:25:55.606970Z","iopub.status.idle":"2022-12-29T12:25:58.349991Z","shell.execute_reply":"2022-12-29T12:25:58.348680Z","shell.execute_reply.started":"2022-12-29T12:25:55.607419Z"},"id":"2tSbGA4iNyJO","trusted":true},"outputs":[],"source":["import optuna \n","from stable_baselines3 import PPO\n","# useful for evaluting the current policy during our hyperparameter tuning\n","from stable_baselines3.common.evaluation import evaluate_policy\n","# import Monitor for logging\n","from stable_baselines3.common.monitor import Monitor\n","# import DummyVecEnv for vectorizing our environment and frame stacking\n","from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.355179Z","iopub.status.busy":"2022-12-29T12:25:58.354624Z","iopub.status.idle":"2022-12-29T12:25:58.361767Z","shell.execute_reply":"2022-12-29T12:25:58.360035Z","shell.execute_reply.started":"2022-12-29T12:25:58.355147Z"},"id":"yNfNJqCaNyJO","trusted":true},"outputs":[],"source":["LOG_DIR = \"./logs/\"\n","OPT_DIR = \"./opt/\""]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.364177Z","iopub.status.busy":"2022-12-29T12:25:58.363690Z","iopub.status.idle":"2022-12-29T12:25:58.376322Z","shell.execute_reply":"2022-12-29T12:25:58.374988Z","shell.execute_reply.started":"2022-12-29T12:25:58.364132Z"},"id":"HVfaL4j4NyJO","trusted":true},"outputs":[],"source":["# Function to return test hyperparameters\n","def optimize_ppo(trial):\n","    return {\n","        \"n_steps\": trial.suggest_int(\"n_steps\", 2048, 8192),\n","        \"gamma\": trial.suggest_loguniform(\"gamma\", 0.8, 0.9999),\n","        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-4),\n","        \"clip_range\": trial.suggest_uniform(\"clip_range\", 0.1, 0.4),\n","        \"gae_lambda\": trial.suggest_uniform(\"gae_lambda\", 0.8, 0.99),\n","    }"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.378380Z","iopub.status.busy":"2022-12-29T12:25:58.378049Z","iopub.status.idle":"2022-12-29T12:25:58.396323Z","shell.execute_reply":"2022-12-29T12:25:58.395245Z","shell.execute_reply.started":"2022-12-29T12:25:58.378352Z"},"id":"F15gAb2QSsGZ","trusted":true},"outputs":[],"source":["env.close()"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.398124Z","iopub.status.busy":"2022-12-29T12:25:58.397776Z","iopub.status.idle":"2022-12-29T12:25:58.411452Z","shell.execute_reply":"2022-12-29T12:25:58.410299Z","shell.execute_reply.started":"2022-12-29T12:25:58.398093Z"},"id":"bdO9ZVwlNyJO","trusted":true},"outputs":[],"source":["# Setup the training loop and return the mean reward\n","total_steps = 100000\n","def train_ppo(trial):\n","    try:\n","        # setup the hyperparameters\n","        hyperparams = optimize_ppo(trial)\n","        # setup the environment\n","        env = StreetFighter()\n","        # setup the monitor (this is important since we are vectorizing the environment, because this allows us \n","        # to get the mean episode reward and mean episode length)\n","        env = Monitor(env, LOG_DIR)\n","        # setup the vectorized environment\n","        env = DummyVecEnv([lambda: env])\n","        # setup the frame stacking\n","        env = VecFrameStack(env, n_stack=4, channels_order='last')\n","        # setup the model\n","        model = PPO(\"CnnPolicy\", env, verbose=0, tensorboard_log=LOG_DIR, **hyperparams)\n","        # train the model\n","        model.learn(total_timesteps=total_steps)\n","        # evaluate the model\n","        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n","        # close the environment\n","        env.close()\n","\n","        # save the best model\n","        SAVE_PATH = os.path.join(OPT_DIR, \"trial_{}_best_model\".format(trial.number))\n","        model.save(SAVE_PATH)\n","\n","        return mean_reward\n","\n","    except Exception as e:\n","        print(e)\n","        return -1000"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-29T12:25:58.414157Z","iopub.status.busy":"2022-12-29T12:25:58.413261Z","iopub.status.idle":"2022-12-29T12:25:58.430347Z","shell.execute_reply":"2022-12-29T12:25:58.428961Z","shell.execute_reply.started":"2022-12-29T12:25:58.414101Z"},"id":"xOEjeIGZNyJP","outputId":"ca368eab-1b12-498e-c9b2-63862afbadcd","trusted":true},"outputs":[],"source":["# NOTE that since we used a positive reward function, we are maximizing the reward\n","# study = optuna.create_study(direction=\"maximize\")\n","# study.optimize(train_ppo, n_trials=100, n_jobs=1)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.433129Z","iopub.status.busy":"2022-12-29T12:25:58.432128Z","iopub.status.idle":"2022-12-29T12:25:58.446316Z","shell.execute_reply":"2022-12-29T12:25:58.444890Z","shell.execute_reply.started":"2022-12-29T12:25:58.433082Z"},"id":"8RGwHB0mNyJP","trusted":true},"outputs":[],"source":["# best_model = PPO.load(os.path.join(OPT_DIR, \"trial_{}_best_model\".format(study.best_trial.number)))"]},{"cell_type":"markdown","metadata":{"id":"mlrTn8ZMNyJP"},"source":["# Setup Callback"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.448547Z","iopub.status.busy":"2022-12-29T12:25:58.447927Z","iopub.status.idle":"2022-12-29T12:25:58.458675Z","shell.execute_reply":"2022-12-29T12:25:58.457270Z","shell.execute_reply.started":"2022-12-29T12:25:58.448512Z"},"id":"up4-0bdNNyJP","trusted":true},"outputs":[],"source":["from stable_baselines3.common.callbacks import BaseCallback"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.462007Z","iopub.status.busy":"2022-12-29T12:25:58.460466Z","iopub.status.idle":"2022-12-29T12:25:58.476066Z","shell.execute_reply":"2022-12-29T12:25:58.474548Z","shell.execute_reply.started":"2022-12-29T12:25:58.461948Z"},"id":"xYtAE4jsNyJQ","trusted":true},"outputs":[],"source":["class TrainAndLoggingCallback(BaseCallback):\n","\n","    def __init__(self, check_freq, save_path, verbose=1):\n","        super(TrainAndLoggingCallback, self).__init__(verbose)\n","        self.check_freq = check_freq\n","        self.save_path = save_path\n","\n","    def _init_callback(self):\n","        if self.save_path is not None:\n","            os.makedirs(self.save_path, exist_ok=True)\n","\n","    def _on_step(self):\n","        if self.n_calls % self.check_freq == 0:\n","            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n","            self.model.save(model_path)\n","\n","        return True"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.478665Z","iopub.status.busy":"2022-12-29T12:25:58.477794Z","iopub.status.idle":"2022-12-29T12:25:58.504137Z","shell.execute_reply":"2022-12-29T12:25:58.501461Z","shell.execute_reply.started":"2022-12-29T12:25:58.478598Z"},"id":"mInrcbCsNyJQ","trusted":true},"outputs":[],"source":["CHECKPOINT_DIR = \"./train/\"\n","callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"]},{"cell_type":"markdown","metadata":{"id":"ZwwB16QmNyJQ"},"source":["# Train Model"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:25:58.507384Z","iopub.status.busy":"2022-12-29T12:25:58.506476Z","iopub.status.idle":"2022-12-29T12:25:58.775392Z","shell.execute_reply":"2022-12-29T12:25:58.774089Z","shell.execute_reply.started":"2022-12-29T12:25:58.507304Z"},"id":"j8MVdSRANyJQ","trusted":true},"outputs":[],"source":["env.close()\n","# Recreate the environment\n","env = StreetFighter()\n","# setup the monitor (this is important since we are vectorizing the environment, because this allows us\n","# to get the mean episode reward and mean episode length)\n","env = Monitor(env, LOG_DIR)\n","# setup the vectorized environment\n","env = DummyVecEnv([lambda: env])\n","# setup the frame stacking\n","env = VecFrameStack(env, n_stack=4, channels_order='last')"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-29T12:25:58.778143Z","iopub.status.busy":"2022-12-29T12:25:58.777270Z","iopub.status.idle":"2022-12-29T12:25:59.074645Z","shell.execute_reply":"2022-12-29T12:25:59.073618Z","shell.execute_reply.started":"2022-12-29T12:25:58.778090Z"},"id":"g-tfme6MNyJR","outputId":"ff4624c4-dc2f-43c6-d1a0-3837ff5231b0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu device\n","Wrapping the env in a VecTransposeImage.\n"]}],"source":["# code that we used to originally train the model\n","# We got these model params from the hyperparameter optimization trials\n","model_params = {'n_steps': 2570.949, 'gamma': 0.906, 'learning_rate': 2e-07, 'clip_range': 0.369, 'gae_lambda': 0.891}\n","model_params['n_steps'] = 40 * 64 # based on optuna study from above (rounding to nearest factor of 64)\n","\n","model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n","# model.learn(total_timesteps=5000000, callback=callback)\n","env.close()"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/plain":["{'n_steps': 2560,\n"," 'gamma': 0.906,\n"," 'learning_rate': 2e-07,\n"," 'clip_range': 0.369,\n"," 'gae_lambda': 0.891}"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["model_params"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:54:34.581513Z","iopub.status.busy":"2022-12-29T12:54:34.580918Z","iopub.status.idle":"2022-12-29T12:54:36.625409Z","shell.execute_reply":"2022-12-29T12:54:36.623970Z","shell.execute_reply.started":"2022-12-29T12:54:34.581458Z"},"trusted":true},"outputs":[],"source":["# # recreate the zip file for the best model so far\n","# import shutil\n","# shutil.make_archive(\"best_model\", 'zip', \"/kaggle/input/street-fighter-rom/best_model_5460000\")\n","# # load the model \n","# model = PPO.load(\"/kaggle/working/best_model\")"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"execution":{"iopub.execute_input":"2022-12-29T12:54:30.947001Z","iopub.status.busy":"2022-12-29T12:54:30.945566Z","iopub.status.idle":"2022-12-29T12:54:30.953077Z","shell.execute_reply":"2022-12-29T12:54:30.951337Z","shell.execute_reply.started":"2022-12-29T12:54:30.946950Z"},"id":"SYTS0HZpYoxG","outputId":"4cdac382-4e91-414e-db4c-bd58066e2d2f","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/jacobjun/Python projects/Street-Fighter-agent/street_fighter/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n","  warnings.warn(\n"]}],"source":["# Load the model from the provided training (to save compute resources)\n","custom_objects = {\n","        \"learning_rate\": 2e-07,\n","        \"clip_range\": lambda _: 0.369,\n","    }\n","model_version = \"best_model_nonoptuna_4\"\n","model = PPO.load(f'./train/{model_version}.zip', custom_objects=custom_objects)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:54:55.661534Z","iopub.status.busy":"2022-12-29T12:54:55.661078Z","iopub.status.idle":"2022-12-29T12:54:55.954967Z","shell.execute_reply":"2022-12-29T12:54:55.953601Z","shell.execute_reply.started":"2022-12-29T12:54:55.661500Z"},"id":"86X4cNoIafZ3","trusted":true},"outputs":[],"source":["env = StreetFighter()\n","env = Monitor(env, LOG_DIR)\n","env = DummyVecEnv([lambda: env])\n","env = VecFrameStack(env, 4, channels_order='last')"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-12-29T12:54:59.040311Z","iopub.status.busy":"2022-12-29T12:54:59.039819Z","iopub.status.idle":"2022-12-29T12:54:59.514549Z","shell.execute_reply":"2022-12-29T12:54:59.511869Z","shell.execute_reply.started":"2022-12-29T12:54:59.040273Z"},"id":"DAPPaMhxNyJR","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-12-31 22:16:52.835 python[79263:2469808] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fba9d238450>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n","2022-12-31 22:16:52.837 python[79263:2469808] Warning: Expected min height of view: (<NSButton: 0x7fba96ef0f70>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n","2022-12-31 22:16:52.844 python[79263:2469808] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fba97846030>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n","2022-12-31 22:16:52.846 python[79263:2469808] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fba97931340>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"]},{"name":"stdout","output_type":"stream","text":["Total Reward for episode 0 is [164400.]\n"]}],"source":["for episode in range(1): \n","    obs = env.reset()\n","    done = False\n","    total_reward = 0\n","    while not done: \n","        action, _ = model.predict(obs)\n","        obs, reward, done, info = env.step(action)\n","        env.render()\n","        # time.sleep(0.01)\n","        total_reward += reward\n","    print('Total Reward for episode {} is {}'.format(episode, total_reward))\n","    time.sleep(2)"]},{"cell_type":"markdown","metadata":{"id":"-83XlWLxNyJR"},"source":["# Test the model"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.status.busy":"2022-12-29T12:25:59.276782Z","iopub.status.idle":"2022-12-29T12:25:59.277328Z","shell.execute_reply":"2022-12-29T12:25:59.277122Z","shell.execute_reply.started":"2022-12-29T12:25:59.277099Z"},"id":"2CTOPrNMNyJS","trusted":true},"outputs":[],"source":["# import time\n","\n","# # code to render the agent's progress and log the rewards\n","# for episode in range(1): \n","#     obs = env.reset()\n","#     done = False\n","#     total_reward = 0\n","#     while not done: \n","#         action, _ = model.predict(obs)\n","#         obs, reward, done, info = env.step(action)\n","#         env.render()\n","#         time.sleep(0.01)\n","#         total_reward += reward\n","#     print('Total Reward for episode {} is {}'.format(total_reward, episode))\n","#     time.sleep(2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Record a video of the current progress"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["# from stable_baselines3.common.vec_env import VecVideoRecorder\n","# import imageio\n","# from IPython.display import HTML\n","# from IPython import display as ipythondisplay\n","# import glob\n","# import io\n","# import base64\n","# from gym.wrappers import Monitor\n","\n","# video_folder = \"./logs/\"\n","# video_length = 350\n","\n","# def create_mp4(model, env):\n","#     model.set_env(env)\n","#     # Record the video starting at the first step\n","#     env = VecVideoRecorder(env, video_folder,\n","#                         record_video_trigger=lambda x: x == 0, video_length=video_length,\n","#                         name_prefix=\"ppo-sf2{}\".format(gamename))\n","\n","#     # update the model's env\n","#     model.set_env(env)\n","#     obs = model.env.reset()\n","#     for _ in range(video_length + 1):\n","#         action = model.predict(obs)\n","#         obs, _, _, _ = model.env.step(action)\n","#     # Save the video\n","#     model.env.close()\n","\n","\n","# def create_gif(model, env):\n","#     model.set_env(env)\n","#     images = []\n","#     obs = model.env.reset()\n","#     img = model.env.render()\n","#     # TODO the current issue is that our render method returns None (which makes sense since the return value is optional)\n","#     print(img)\n","#     for i in range(350):\n","#         images.append(img)\n","#         action, _ = model.predict(obs)\n","#         obs, _, _ ,_ = model.env.step(action)\n","#         img = model.env.render()\n","#         print(img)\n","\n","#     imageio.mimsave(f'ppo_sf2_{model_version}.gif', [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=60)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["# create_mp4(model, env)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"street_fighter","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"vscode":{"interpreter":{"hash":"9c3846ce54bed1895ba0d2c90c8f55d61bce045021babe443836d90a30036b65"}}},"nbformat":4,"nbformat_minor":4}
